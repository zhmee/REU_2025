{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4efb0-f18e-46c5-8a19-0ca88b6a43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8867195-b847-4db4-8db4-86159cbf0e6a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5341d-5342-412a-afdd-8da346aae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c451-25a7-4e3d-a312-a3fe640dbcc0",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32caf1b6-ed6b-44cb-81af-d27522f0ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../ABI_Data'\n",
    "data_load_from_file = False\n",
    "pickle_path = './dataframe.pkl'\n",
    "pickle_create = False\n",
    "pickle_load = False\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "train_proportion = .8\n",
    "val_proportion = .1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fb76f-cfba-4a4a-b6a2-041423df675c",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0e7fa-0e4c-411b-86a4-deab428c6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_load_from_file or pickle_create:\n",
    "    print(\"Files start\")\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith('.npz')][:100]\n",
    "    print(\"Files done\")\n",
    "    \n",
    "    records = []\n",
    "    print(\"Records start\")\n",
    "    for file in tqdm(files):\n",
    "        path = os.path.join(data_dir, file)\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            records.extend([{k: arr[i] for k, arr in data.items()} for i in range(len(next(iter(data.values()))))])\n",
    "            # print(f\"Loaded: {file}\")\n",
    "            del data\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load: {file} : {e}\")\n",
    "            pass\n",
    "    \n",
    "    print(\"DF start\")\n",
    "    df = pd.DataFrame(records)\n",
    "    print(\"DF done\")\n",
    "    if pickle_create:\n",
    "        print(\"Starting pickle\")\n",
    "        df.to_pickle(pickle_path)\n",
    "        print(f\"Pickle saved to: {pickle_path}\")\n",
    "    del files, records\n",
    "if pickle_load:\n",
    "    print(\"Loading Pickle\")\n",
    "    df = pd.read_pickle('dataframe.pkl')\n",
    "    print(\"Pickle Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97048872-e33c-4ba1-b9df-54d78b500ea3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Leave this as backup\n",
    "\n",
    "# if data_load_from_file or pickle_create:\n",
    "#     print(\"Files start\")\n",
    "#     files = [f for f in os.listdir(data_dir) if f.endswith('.npz')][:100]\n",
    "#     print(\"Files done\")\n",
    "    \n",
    "#     records = []\n",
    "#     print(\"Records start\")\n",
    "#     for file in tqdm(files):\n",
    "#         path = os.path.join(data_dir, file)\n",
    "#         try:\n",
    "#             data = np.load(path)\n",
    "#             records.extend([{k: arr[i] for k, arr in data.items()} for i in range(len(next(iter(data.values()))))])\n",
    "#             # print(f\"Loaded: {file}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load: {file} : {e}\")\n",
    "#             pass\n",
    "    \n",
    "#     print(\"DF start\")\n",
    "#     df = pd.DataFrame(records)\n",
    "#     print(\"DF done\")\n",
    "#     if pickle_create:\n",
    "#         print(\"Starting pickle\")\n",
    "#         df.to_pickle(pickle_path)\n",
    "#         print(f\"Pickle saved to: {pickle_path}\")\n",
    "        \n",
    "# if pickle_load:\n",
    "#     print(\"Loading Pickle\")\n",
    "#     df = pd.read_pickle('dataframe.pkl')\n",
    "#     print(\"Pickle Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea9ac0-651b-4344-99e0-ce0477a7194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_column_info(df):\n",
    "#     for col in df.columns:\n",
    "#         dtype = df[col].dtype\n",
    "#         sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else \"NaN\"\n",
    "#         print(f\"{col}: {dtype}, sample: {sample}\")\n",
    "# print_column_info(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0644fa-97be-491e-8899-be3bc42e79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=['l2_cloud_mask', 'l2_cloud_phase', 'l2_cod', 'l2_cps'])\n",
    "# print(f\"Remaining samples after dropping NaNs: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ae380-778e-4d37-b33f-d0b5ea76b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(df):\n",
    "#     # Inputs: stack rad arrays (N samples, 128 rows, 16 channels)\n",
    "#     # We want shape (N, channels=16, height=128)\n",
    "#     rad_np = np.stack(df['rad'].values)  # shape (N, 128, 16)\n",
    "#     X = torch.tensor(rad_np, dtype=torch.float32).permute(0, 2, 1)  # (N, 16, 128)\n",
    "    \n",
    "#     # Targets: stack and convert\n",
    "#     y_mask = torch.tensor(np.stack(df['l2_cloud_mask'].values), dtype=torch.long)    # (N, 128)\n",
    "#     y_phase = torch.tensor(np.stack(df['l2_cloud_phase'].values), dtype=torch.long)  # (N, 128)\n",
    "#     y_cod = torch.tensor(np.stack(df['l2_cod'].values), dtype=torch.float32)         # (N, 128)\n",
    "#     y_cps = torch.tensor(np.stack(df['l2_cps'].values), dtype=torch.float32)         # (N, 128)\n",
    "    \n",
    "#     return X, y_mask, y_phase, y_cod, y_cps\n",
    "\n",
    "# X, y_mask, y_phase, y_cod, y_cps = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c83796-60a6-43f8-a083-4352f21fcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, in_channels=16, seq_length=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Individual heads with extra layers\n",
    "        self.mask_head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(128, 2, kernel_size=1)  # 2 classes: mask / no-mask\n",
    "        )\n",
    "        \n",
    "        self.phase_head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(128, 5, kernel_size=1)  # 5 classes\n",
    "        )\n",
    "        \n",
    "        self.cod_head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=1)  # Regression\n",
    "        )\n",
    "        \n",
    "        self.cps_head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=1)  # Regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared backbone\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        # Individual heads\n",
    "        mask_logits = self.mask_head(x)\n",
    "        phase_logits = self.phase_head(x)\n",
    "        cod_pred = self.cod_head(x).squeeze(1)\n",
    "        cps_pred = self.cps_head(x).squeeze(1)\n",
    "    \n",
    "        mask_pred = mask_logits.argmax(dim=1)\n",
    "    \n",
    "        mask_filter = (mask_pred == 1).float()\n",
    "    \n",
    "        phase_logits = phase_logits * mask_filter.unsqueeze(1)\n",
    "        cod_pred = cod_pred * mask_filter\n",
    "        cps_pred = cps_pred * mask_filter\n",
    "    \n",
    "        return mask_logits, phase_logits, cod_pred, cps_pred\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Shared backbone\n",
    "    #     x = F.relu(self.bn1(self.conv1(x)))\n",
    "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
    "    #     x = F.relu(self.bn3(self.conv3(x)))\n",
    "    #     x = self.dropout(x)\n",
    "\n",
    "    #     # Individual heads\n",
    "    #     mask_logits = self.mask_head(x)\n",
    "    #     phase_logits = self.phase_head(x)\n",
    "    #     cod_pred = self.cod_head(x).squeeze(1)\n",
    "    #     cps_pred = self.cps_head(x).squeeze(1)\n",
    "\n",
    "    #     return mask_logits, phase_logits, cod_pred, cps_pred        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99347427-6716-4ee9-b324-5647695d24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_zero = df[df['l2_cloud_mask'].apply(lambda x: 0.0 in x)].head(50)\n",
    "# x = rows_with_zero.iloc[0,:]\n",
    "\n",
    "\n",
    "\n",
    "# mask = np.array(x['l2_cloud_mask'])\n",
    "# zero_indices = np.where(mask == 0)[0]\n",
    "# print(zero_indices)\n",
    "# mask_zero = phase[zero_indices]\n",
    "\n",
    "# print(mask_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fc97d-9724-442a-9674-45efb4a8f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_zero = df[df['l2_cloud_mask'].apply(lambda x: 0.0 in x)].head(5)\n",
    "\n",
    "# for idx, row in rows_with_zero.iterrows():\n",
    "#     # Convert to NumPy arrays (in case they're lists)\n",
    "#     mask = np.array(row['l2_cloud_mask'])\n",
    "#     phase = np.array(row['l2_cloud_phase'])\n",
    "#     cod = np.array(row['l2_cod'])\n",
    "#     cps = np.array(row['l2_cps'])\n",
    "\n",
    "#     # Find where mask == 0\n",
    "#     zero_indices = np.where(mask == 0)[0]\n",
    "\n",
    "#     # Extract corresponding values\n",
    "#     mask_zero = phase[zero_indices]\n",
    "#     phase_zero = phase[zero_indices]\n",
    "#     cod_zero = cod[zero_indices]\n",
    "#     cps_zero = cps[zero_indices]\n",
    "\n",
    "#     print(f\"\\nRow {idx} — {len(zero_indices)} cloud-free points:\")\n",
    "#     print(\"  l2_cloud_mask: \", mask_zero)\n",
    "#     print(\"  l2_cloud_phase:\", phase_zero)\n",
    "#     print(\"  l2_cod:        \", cod_zero)\n",
    "#     print(\"  l2_cps:        \", cps_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b51bd-619a-48c1-a5f0-193b652f4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_loss_fn = nn.CrossEntropyLoss()   # expects shape (N, C, L), targets shape (N, L)\n",
    "phase_loss_fn = nn.CrossEntropyLoss()\n",
    "cod_loss_fn = nn.MSELoss()\n",
    "cps_loss_fn = nn.MSELoss()\n",
    "\n",
    "def multitask_loss(outputs, targets, weights=None):\n",
    "    mask_logits, phase_logits, cod_pred, cps_pred = outputs\n",
    "    mask_target, phase_target, cod_target, cps_target = targets\n",
    "        \n",
    "    # CrossEntropyLoss expects target shape (N, L) with class indices (long)\n",
    "    loss_mask = mask_loss_fn(mask_logits, mask_target.long())\n",
    "    loss_phase = phase_loss_fn(phase_logits, phase_target.long())\n",
    "    \n",
    "    # Regression losses: input and target shape (N, L), float\n",
    "    loss_cod = cod_loss_fn(cod_pred, cod_target)\n",
    "    loss_cps = cps_loss_fn(cps_pred, cps_target)\n",
    "    \n",
    "    # Combine losses with optional weighting\n",
    "    if weights is None:\n",
    "        weights = [1.0, 1.0, 1.0, 1.0]\n",
    "    \n",
    "    total_loss = weights[0]*loss_mask + weights[1]*loss_phase + weights[2]*loss_cod + weights[3]*loss_cps\n",
    "    return total_loss, (loss_mask, loss_phase, loss_cod, loss_cps)\n",
    "\n",
    "def compute_accuracy(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=1)  # shape: (batch, seq_len)\n",
    "    correct = (preds == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def compute_metrics(outputs, targets):\n",
    "    mask_logits, phase_logits, cod_pred, cps_pred = outputs\n",
    "    y_mask, y_phase, y_cod, y_cps = targets\n",
    "\n",
    "    # Get predicted classes\n",
    "    mask_preds = torch.argmax(mask_logits, dim=1).detach().cpu().numpy()\n",
    "    phase_preds = torch.argmax(phase_logits, dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    # True classes\n",
    "    y_mask_np = y_mask.detach().cpu().numpy()\n",
    "    y_phase_np = y_phase.detach().cpu().numpy()\n",
    "    \n",
    "    # Classification accuracy\n",
    "    mask_acc = accuracy_score(y_mask_np.flatten(), mask_preds.flatten())\n",
    "    phase_acc = accuracy_score(y_phase_np.flatten(), phase_preds.flatten())\n",
    "\n",
    "    # Regression metrics\n",
    "    cod_mae = mean_absolute_error(y_cod.detach().cpu().numpy().flatten(), cod_pred.detach().cpu().numpy().flatten())\n",
    "    cps_mae = mean_absolute_error(y_cps.detach().cpu().numpy().flatten(), cps_pred.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return mask_acc, phase_acc, cod_mae, cps_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a36fc-e388-4761-96ad-7989b0593796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "        # # Filter samples where all dqf flags equal 1 inside arrays\n",
    "        # valid_mask = df['l2_dqf_cloud_mask'].apply(lambda arr: (arr == 1).all())\n",
    "        # valid_phase = df['l2_dqf_cloud_phase'].apply(lambda arr: (arr == 1).all())\n",
    "        # valid_cod = df['l2_dqf_cod'].apply(lambda arr: (arr == 1).all())\n",
    "        # valid_cps = df['l2_dqf_cps'].apply(lambda arr: (arr == 1).all())\n",
    "\n",
    "        # valid_idx = valid_mask & valid_phase & valid_cod & valid_cps\n",
    "\n",
    "        # self.df = df[valid_idx].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get rad input: shape (128, 16) -> transpose to (16, 128)\n",
    "        rad = self.df.loc[idx, 'rad'].astype('float32').T  # final shape: (16, 128)\n",
    "        \n",
    "        # Targets (all shape (128,))\n",
    "        mask = self.df.loc[idx, 'l2_cloud_mask'].astype('int64')\n",
    "        phase = self.df.loc[idx, 'l2_cloud_phase'].astype('int64')\n",
    "        cod = self.df.loc[idx, 'l2_cod'].astype('float32')\n",
    "        cps = self.df.loc[idx, 'l2_cps'].astype('float32')\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        rad_t = torch.tensor(rad)       # shape: (16, 128)\n",
    "        mask_t = torch.tensor(mask)     # shape: (128,)\n",
    "        phase_t = torch.tensor(phase)   # shape: (128,)\n",
    "        cod_t = torch.tensor(cod)       # shape: (128,)\n",
    "        cps_t = torch.tensor(cps)       # shape: (128,)\n",
    "        \n",
    "        return rad_t, mask_t, phase_t, cod_t, cps_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd3c9-85ad-4783-bec5-7e56df323e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiTaskDataset(df)\n",
    "model = MultiTaskModel()\n",
    "model.to(device)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_weights = [1.0,1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce7e9a-9160-47d5-ba61-b0e6e81c2a2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "epoch_mask_acc = []\n",
    "epoch_phase_acc = []\n",
    "epoch_cod_mae = []\n",
    "epoch_cps_mae = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    all_mask_acc = []\n",
    "    all_phase_acc = []\n",
    "    all_cod_mae = []\n",
    "    all_cps_mae = []\n",
    "\n",
    "    for X, y_mask, y_phase, y_cod, y_cps in loader:\n",
    "        X, y_mask, y_phase, y_cod, y_cps = X.to(device), y_mask.to(device), y_phase.to(device), y_cod.to(device), y_cps.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        targets = (y_mask, y_phase, y_cod, y_cps)\n",
    "\n",
    "        loss, _ = multitask_loss(outputs, targets, weights=loss_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Metrics\n",
    "        mask_acc, phase_acc, cod_mae, cps_mae = compute_metrics(outputs, targets)\n",
    "        all_mask_acc.append(mask_acc)\n",
    "        all_phase_acc.append(phase_acc)\n",
    "        all_cod_mae.append(cod_mae)\n",
    "        all_cps_mae.append(cps_mae)\n",
    "\n",
    "\n",
    "\n",
    "    # Epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {total_loss:.4f}, \"\n",
    "          f\"Mask Acc: {np.mean(all_mask_acc):.4f}, \"\n",
    "          f\"Phase Acc: {np.mean(all_phase_acc):.4f}, \"\n",
    "          f\"COD MAE: {np.mean(all_cod_mae):.2f}, \"\n",
    "          f\"CPS MAE: {np.mean(all_cps_mae):.2f}\")\n",
    "    epoch_losses.append(total_loss)\n",
    "    epoch_mask_acc.append(np.mean(all_mask_acc))\n",
    "    epoch_phase_acc.append(np.mean(all_phase_acc))\n",
    "    epoch_cod_mae.append(np.mean(all_cod_mae))\n",
    "    epoch_cps_mae.append(np.mean(all_cps_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085b52e-f59a-4f44-9cba-e38ccaed8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, num_epochs + 1))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Accuracy subplot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, epoch_mask_acc, label='Mask Accuracy')\n",
    "plt.plot(epochs, epoch_phase_acc, label='Phase Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(1, num_epochs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# MAE subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, epoch_cod_mae, label='COD MAE')\n",
    "plt.plot(epochs, epoch_cps_mae, label='CPS MAE')\n",
    "plt.xlim(1, num_epochs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Regression MAE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/Caleb/multi-task-v3.png\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
