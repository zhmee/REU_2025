{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54f56c2-9e56-4c43-8ac7-1632968c4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "#File IO\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#Data manipulation\n",
    "import numpy as np\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "#Misc\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f637138-7a72-4e4b-b66d-c4129a08e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TARGET DATA\n",
    "\n",
    "def get_features(file):\n",
    "    with np.load(file) as data:\n",
    "        bands = data['rad']\n",
    "        features = np.stack([\n",
    "            data['l2_cloud_mask'],\n",
    "            data['l2_cloud_phase'],\n",
    "            np.log1p(data['l2_cod']),  # log(1 + cod)\n",
    "            np.log1p(data['l2_cps'])   # log(1 + cps)\n",
    "        ])\n",
    "    return bands, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498e7a43-417c-4e1c-ac5d-e6c39ab9c876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABI_Data_20231451250225_1577.npz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_files = ['/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231451250225_1577.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231482000207_932.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231111440210_186.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231701140225_465.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231541010218_855.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231470940228_463.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231491330208_399.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231051950204_363.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231241550212_1569.npz']\n",
    "\n",
    "shared_files = [f[80:] for f in shared_files]\n",
    "\n",
    "shared_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef86de48-69ca-494e-ac10-c2699c322110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABI_Data_20231541040218_855.npz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob('../ABI_Data_Scaled/*.npz')\n",
    "file_list[0][19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af37ff9-2fb5-4cc9-9659-cc8230b087e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "563it [00:38, 14.73it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m y = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(file_list)):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     features, target = \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     X.append(features)\n\u001b[32m     13\u001b[39m     y.append(target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_features\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_features\u001b[39m(file):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m np.load(file) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         bands = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrad\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m         features = np.stack([\n\u001b[32m      7\u001b[39m             data[\u001b[33m'\u001b[39m\u001b[33ml2_cloud_mask\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m             data[\u001b[33m'\u001b[39m\u001b[33ml2_cloud_phase\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m             np.log1p(data[\u001b[33m'\u001b[39m\u001b[33ml2_cod\u001b[39m\u001b[33m'\u001b[39m]),  \u001b[38;5;66;03m# log(1 + cod)\u001b[39;00m\n\u001b[32m     10\u001b[39m             np.log1p(data[\u001b[33m'\u001b[39m\u001b[33ml2_cps\u001b[39m\u001b[33m'\u001b[39m])   \u001b[38;5;66;03m# log(1 + cps)\u001b[39;00m\n\u001b[32m     11\u001b[39m         ])\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bands, features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:257\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mbytes\u001b[39m.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# FIXME: This seems like it will copy strings around\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m#   more than is strictly necessary.  The zipfile\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m#   (or at least uncompress) the data\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m#   directly into the array memory.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m.read(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/site-packages/numpy/lib/_format_impl.py:869\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    867\u001b[39m             read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    868\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m             data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m             array[i:i + read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    871\u001b[39m                                                      count=read_count)\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m array.size != count:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/site-packages/numpy/lib/_format_impl.py:1013\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1009\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m         data += r\n\u001b[32m   1015\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/zipfile/__init__.py:992\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    994\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/zipfile/__init__.py:1062\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1060\u001b[39m         data += \u001b[38;5;28mself\u001b[39m._read2(n - \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_STORED:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/zipfile/__init__.py:1092\u001b[39m, in \u001b[36mZipExtFile._read2\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1089\u001b[39m n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m   1090\u001b[39m n = \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m._compress_left)\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28mself\u001b[39m._compress_left -= \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/umbc/rs/cybertrn/reu2025/team1/research/Model1/envs/jupyter_env/lib/python3.12/zipfile/__init__.py:811\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt read from the ZIP file while there \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    808\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis an open writing handle on it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    809\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mClose the writing handle before trying to read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    810\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "max_images = 15000\n",
    "\n",
    "#CREATE DATASET\n",
    "\n",
    "file_list = glob.glob('../ABI_Data_Scaled/*.npz')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_list)):\n",
    "    features, target = get_features(file)\n",
    "    X.append(features)\n",
    "    y.append(target)\n",
    "    if (file[19:] in shared_files):\n",
    "        print(f\"Shared file: {file}\")\n",
    "        continue\n",
    "    if i == max_images-1: break\n",
    "\n",
    "X = np.stack(X)  # shape: (Samples, 128, 128, 16)\n",
    "X = torch.from_numpy(X).float()\n",
    "X = X.permute(0, 3, 1, 2)  # shape: (Samples, 16, 128, 128)\n",
    "\n",
    "\n",
    "y = np.stack(y)\n",
    "y = torch.from_numpy(y).float() # shape: (Samples, 4, 128, 128)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "torch.save(dataset, \"dataset_scaled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e21cb-f136-4e1e-acf4-186bbe91dfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
