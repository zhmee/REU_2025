{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64630725-a4ec-45f7-a3cc-a9e874647ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "#File IO\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast\n",
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "#Scikit learn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Misc\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0395c608-61b2-4410-a023-e8b7b4eff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARMETERS\n",
    "\n",
    "train_proportion = .8\n",
    "val_proportion = .1\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = .00002\n",
    "num_epochs = 100\n",
    "loss_weights = (1.0, 1.0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160b26c5-f145-47d8-8734-5d5949fd48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2330it [00:04, 574.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231111440210_186.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6383it [00:11, 580.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231491330208_399.npz\n",
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231541010218_855.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8033it [00:14, 586.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231482000207_932.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8690it [00:15, 581.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231470940228_463.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9705it [00:16, 588.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231051950204_363.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12673it [00:21, 583.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231451250225_1577.npz\n",
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231241550212_1569.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13382it [00:23, 574.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared file: ../ABI_Data_Scaled/ABI_Data_20231701140225_465.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14973it [00:25, 577.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# #LOAD DATASET\n",
    "# shared_files = ['/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231451250225_1577.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231482000207_932.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231111440210_186.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231701140225_465.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231541010218_855.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231470940228_463.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231491330208_399.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231051950204_363.npz', '/umbc/rs/nasa-access/users/xingyan/pytorch-caney/data/downstream_2dcloud_scaled/ABI_Data_20231241550212_1569.npz']\n",
    "# shared_files = [f[80:] for f in shared_files]\n",
    "\n",
    "\n",
    "# #GET TARGET DATA\n",
    "\n",
    "# def get_features(file):\n",
    "#     with np.load(file) as data:\n",
    "#         bands = data['rad']\n",
    "#         features = np.stack([\n",
    "#             data['l2_cloud_mask'],\n",
    "#             data['l2_cloud_phase'],\n",
    "#             data['l2_cod'] * 0.001,\n",
    "#             data['l2_cps'] * 0.001\n",
    "#         ])\n",
    "#     return bands, features\n",
    "\n",
    "# max_images = 15000\n",
    "\n",
    "# #CREATE DATASET\n",
    "\n",
    "# file_list = glob.glob('../ABI_Data_Scaled/*.npz')\n",
    "\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for i, file in tqdm(enumerate(file_list)):\n",
    "#     features, target = get_features(file)\n",
    "#     X.append(features)\n",
    "#     y.append(target)\n",
    "#     if (file[19:] in shared_files):\n",
    "#         print(f\"Shared file: {file}\")\n",
    "#         continue\n",
    "#     if i == max_images-1: break\n",
    "\n",
    "# X = np.stack(X)  # shape: (Samples, 128, 128, 16)\n",
    "# X = torch.from_numpy(X).float()\n",
    "# X = X.permute(0, 3, 1, 2)  # shape: (Samples, 16, 128, 128)\n",
    "\n",
    "\n",
    "# y = np.stack(y)\n",
    "# y = torch.from_numpy(y).float() # shape: (Samples, 4, 128, 128)\n",
    "\n",
    "# dataset = TensorDataset(X, y)\n",
    "\n",
    "\n",
    "\n",
    "dataset = torch.load(\"dataset.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ab9303-bf7d-495a-ac4a-09ee15803441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATALOADERS\n",
    "\n",
    "train_size = int(train_proportion * len(dataset))\n",
    "val_size = int(val_proportion * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41cf152-f4a7-470c-abb0-b09c8c1805e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE MODEL\n",
    "\n",
    "class MultiTaskV3_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cloud_mask_net = Unet(\n",
    "            encoder_name='resnet34',\n",
    "            in_channels=64,\n",
    "            classes=1,\n",
    "        )\n",
    "\n",
    "        self.cloud_phase_net = Unet(\n",
    "            encoder_name='resnet34',\n",
    "            in_channels=64,\n",
    "            classes=15,\n",
    "        )\n",
    "        self.cod_net = Unet(\n",
    "            encoder_name='resnet34',\n",
    "            in_channels=64,\n",
    "            classes=15,\n",
    "        )\n",
    "        self.cps_net = Unet(\n",
    "            encoder_name='resnet34',\n",
    "            in_channels=64,\n",
    "            classes=15,\n",
    "        )\n",
    "\n",
    "        self.norm0 = nn.BatchNorm2d(num_features=64)        \n",
    "        self.conv0 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=16, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm0(self.conv0(x)))\n",
    "        cloud_mask_pred = self.cloud_mask_net(x)\n",
    "        cloud_phase_pred = self.cloud_phase_net(x)\n",
    "        cod_pred = self.cod_net(x)\n",
    "        cps_pred = self.cps_net(x)\n",
    "\n",
    "        cloud_phase_pred = torch.cat([cloud_mask_pred, cloud_phase_pred], dim=1)\n",
    "        cod_pred = torch.cat([cloud_mask_pred, cod_pred], dim=1)\n",
    "        cps_pred = torch.cat([cloud_mask_pred, cps_pred], dim=1)\n",
    "\n",
    "        cloud_phase_pred = self.conv1(cloud_phase_pred)\n",
    "        cod_pred = F.relu(self.conv2(cod_pred))\n",
    "        cps_pred = F.relu(self.conv3(cps_pred))\n",
    "\n",
    "        return cloud_mask_pred, cloud_phase_pred, cod_pred, cps_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17aa825-c73b-41d3-b58e-2e0c398572e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL SETUP\n",
    "\n",
    "dev_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(dev_str)\n",
    "model = MultiTaskV3_1().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "scaler = GradScaler(device = device)\n",
    "\n",
    "def unpack_labels(labels):\n",
    "    return (\n",
    "        labels[:, 0:1, :, :],             # cloud_mask → [B, H, W]\n",
    "        labels[:, 1, :, :].long(),        # cloud_phase → [B, H, W]\n",
    "        labels[:, 2:3, :, :],             # cod → [B, 1, H, W]\n",
    "        labels[:, 3:4, :, :]              # cps → [B, 1, H, W]\n",
    "    )\n",
    "\n",
    "\n",
    "train_mask_losses, train_phase_losses, train_cod_losses, train_cps_losses, train_all_losses = [], [], [], [], []\n",
    "val_mask_losses, val_phase_losses, val_cod_losses, val_cps_losses, val_all_losses = [], [], [], [], []\n",
    "\n",
    "train_mask_acc, train_phase_acc, train_cod_r2, train_cps_r2 = [], [], [], []\n",
    "val_mask_acc, val_phase_acc, val_cod_r2, val_cps_r2 = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681ab853-08f1-4a6c-b142-87d8a4706e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN and EVALUATE FUNCTIONS\n",
    "\n",
    "def train(model, train_loader, loss_weights=(1,1,1,1)):\n",
    "    model.train()\n",
    "\n",
    "    total_instances = 0 # Count number of instances in the epoch\n",
    "    total_loss = total_loss_mask = total_loss_phase = total_loss_cod = total_loss_cps = 0 # Total loss and sublosses\n",
    "    mask_correct = phase_correct = 0 # Number of correct guesses for cloud_mask and cloud_phase\n",
    "    cod_preds, cod_labels = [], [] # Cod labels and predictions for calculating r2 \n",
    "    cps_preds, cps_labels = [], [] # Cps labels and predictions for calculating r2\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        cloud_mask_target, cloud_phase_target, cod_target, cps_target = unpack_labels(labels) # Get individual targets\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        B, _, H, W = labels.shape\n",
    "        total_instances += (B * H * W) # Count instances in batch\n",
    "        \n",
    "        with autocast(device_type=dev_str):\n",
    "            preds = model(inputs) # Model predictions\n",
    "\n",
    "            # Loss in batch\n",
    "            loss_mask = nn.BCEWithLogitsLoss()(preds[0], cloud_mask_target)\n",
    "            loss_phase = nn.CrossEntropyLoss()(preds[1], cloud_phase_target)\n",
    "            loss_cod = nn.MSELoss()(preds[2], cod_target)\n",
    "            loss_cps = nn.MSELoss()(preds[3], cps_target)\n",
    "            total_batch_loss = (\n",
    "                loss_weights[0] * loss_mask +\n",
    "                loss_weights[1] * loss_phase +\n",
    "                loss_weights[2] * loss_cod +\n",
    "                loss_weights[3] * loss_cps\n",
    "            )\n",
    "\n",
    "        # Get correct guesses for mask and phase\n",
    "        mask_preds = (torch.sigmoid(preds[0]) > 0.5).long()\n",
    "        mask_correct += (mask_preds == cloud_mask_target).sum().item()\n",
    "        phase_preds = torch.argmax(preds[1], dim=1)\n",
    "        phase_correct += (phase_preds == cloud_phase_target).sum().item()\n",
    "\n",
    "        # Get predicted and actual cod and cps\n",
    "        cod_preds.append(preds[2].cpu().detach().numpy())\n",
    "        cod_labels.append(cod_target.cpu().numpy())\n",
    "        cps_preds.append(preds[3].cpu().detach().numpy())\n",
    "        cps_labels.append(cps_target.cpu().numpy())\n",
    "            \n",
    "\n",
    "        # Update model\n",
    "        scaler.scale(total_batch_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Loss in epoch\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_loss_mask += loss_mask.item()\n",
    "        total_loss_phase += loss_phase.item()\n",
    "        total_loss_cod += loss_cod.item()\n",
    "        total_loss_cps += loss_cps.item()\n",
    "\n",
    "    # ------------------------------------ BATCH LOOP END -------------------------------------------------------------\n",
    "\n",
    "    #Accuracy for mask and phase\n",
    "    mask_accuracy = mask_correct/total_instances\n",
    "    phase_accuracy = phase_correct/total_instances\n",
    "\n",
    "    #Cod R2\n",
    "    cod_preds = np.concatenate(cod_preds).ravel()\n",
    "    cod_labels = np.concatenate(cod_labels).ravel()\n",
    "    cod_r2 = r2_score(cod_labels, cod_preds)\n",
    "\n",
    "    #Cps R2\n",
    "    cps_preds = np.concatenate(cps_preds).ravel()\n",
    "    cps_labels = np.concatenate(cps_labels).ravel()\n",
    "    cps_r2 = r2_score(cps_labels, cps_preds)\n",
    "\n",
    "    return {\n",
    "        'loss_total': total_loss / len(train_loader),\n",
    "        'loss_mask': total_loss_mask / len(train_loader),\n",
    "        'loss_phase': total_loss_phase / len(train_loader),\n",
    "        'loss_cod': total_loss_cod / len(train_loader),\n",
    "        'loss_cps': total_loss_cps / len(train_loader),\n",
    "        'acc_mask': mask_accuracy,\n",
    "        'acc_phase': phase_accuracy,\n",
    "        'r2_cod': cod_r2,\n",
    "        'r2_cps': cps_r2\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, val_loader, loss_weights=(1,1,1,1)):\n",
    "    model.eval()\n",
    "\n",
    "    total_instances = 0 # Count number of instances in the epoch\n",
    "    total_loss = total_loss_mask = total_loss_phase = total_loss_cod = total_loss_cps = 0 # Total loss and sublosses\n",
    "    mask_correct = phase_correct = 0 # Number of correct guesses for cloud_mask and cloud_phase\n",
    "    cod_preds, cod_labels = [], [] # Cod labels and predictions for calculating r2 \n",
    "    cps_preds, cps_labels = [], [] # Cps labels and predictions for calculating r2\n",
    "\n",
    "    for inputs, labels in (val_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        cloud_mask_target, cloud_phase_target, cod_target, cps_target = unpack_labels(labels) # Get individual targets\n",
    "\n",
    "        B, _, H, W = labels.shape\n",
    "        total_instances += (B * H * W) # Count instances in batch\n",
    "        \n",
    "        \n",
    "        with torch.no_grad(), autocast(device_type=dev_str):\n",
    "            preds = model(inputs) # Model predictions\n",
    "\n",
    "            # Loss in batch\n",
    "            loss_mask = nn.BCEWithLogitsLoss()(preds[0], cloud_mask_target)\n",
    "            loss_phase = nn.CrossEntropyLoss()(preds[1], cloud_phase_target)\n",
    "            loss_cod = nn.MSELoss()(preds[2], cod_target)\n",
    "            loss_cps = nn.MSELoss()(preds[3], cps_target)\n",
    "            total_batch_loss = (\n",
    "                loss_weights[0] * loss_mask +\n",
    "                loss_weights[1] * loss_phase +\n",
    "                loss_weights[2] * loss_cod +\n",
    "                loss_weights[3] * loss_cps\n",
    "            )\n",
    "\n",
    "        # Get correct guesses for mask and phase\n",
    "        mask_preds = (torch.sigmoid(preds[0]) > 0.5).long()\n",
    "        mask_correct += (mask_preds == cloud_mask_target).sum().item()\n",
    "        phase_preds = torch.argmax(preds[1], dim=1)\n",
    "        phase_correct += (phase_preds == cloud_phase_target).sum().item()\n",
    "\n",
    "        # Get predicted and actual cod and cps\n",
    "        cod_preds.append(preds[2].cpu().detach().numpy())\n",
    "        cod_labels.append(cod_target.cpu().numpy())\n",
    "        cps_preds.append(preds[3].cpu().detach().numpy())\n",
    "        cps_labels.append(cps_target.cpu().numpy())\n",
    "            \n",
    "        # Loss in epoch\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_loss_mask += loss_mask.item()\n",
    "        total_loss_phase += loss_phase.item()\n",
    "        total_loss_cod += loss_cod.item()\n",
    "        total_loss_cps += loss_cps.item()\n",
    "\n",
    "    # ------------------------------------ BATCH LOOP END -------------------------------------------------------------\n",
    "\n",
    "    #Accuracy for mask and phase\n",
    "    mask_accuracy = mask_correct/total_instances\n",
    "    phase_accuracy = phase_correct/total_instances\n",
    "\n",
    "    #Cod R2\n",
    "    cod_preds = np.concatenate(cod_preds).ravel()\n",
    "    cod_labels = np.concatenate(cod_labels).ravel()\n",
    "    cod_r2 = r2_score(cod_labels, cod_preds)\n",
    "\n",
    "    #Cps R2\n",
    "    cps_preds = np.concatenate(cps_preds).ravel()\n",
    "    cps_labels = np.concatenate(cps_labels).ravel()\n",
    "    cps_r2 = r2_score(cps_labels, cps_preds)\n",
    "\n",
    "    return {\n",
    "        'loss_total': total_loss / len(val_loader),\n",
    "        'loss_mask': total_loss_mask / len(val_loader),\n",
    "        'loss_phase': total_loss_phase / len(val_loader),\n",
    "        'loss_cod': total_loss_cod / len(val_loader),\n",
    "        'loss_cps': total_loss_cps / len(val_loader),\n",
    "        'acc_mask': mask_accuracy,\n",
    "        'acc_phase': phase_accuracy,\n",
    "        'r2_cod': cod_r2,\n",
    "        'r2_cps': cps_r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8eb5e5c-b5e1-44c8-b971-bb5c0d21f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:28<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 2.2340 | Val Loss: 1.9372 | Val Mask: 0.8326 | Val Phase: 0.4237 | Val Cod: -16.3383 | Val Cps: -9.7207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:27<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 1.7201 | Val Loss: 1.5420 | Val Mask: 0.9060 | Val Phase: 0.7018 | Val Cod: -6.7206 | Val Cps: -3.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:26<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 1.3848 | Val Loss: 1.2633 | Val Mask: 0.9231 | Val Phase: 0.7636 | Val Cod: -6.2151 | Val Cps: -1.6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:27<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 1.1335 | Val Loss: 1.0443 | Val Mask: 0.9304 | Val Phase: 0.7988 | Val Cod: -9.9462 | Val Cps: -1.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:26<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.9663 | Val Loss: 0.9123 | Val Mask: 0.9327 | Val Phase: 0.8168 | Val Cod: -17.3253 | Val Cps: -0.5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:26<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.8553 | Val Loss: 0.8401 | Val Mask: 0.9416 | Val Phase: 0.8185 | Val Cod: -3.4463 | Val Cps: -0.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:26<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.7848 | Val Loss: 0.7729 | Val Mask: 0.9417 | Val Phase: 0.8245 | Val Cod: -9.7699 | Val Cps: -0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:27<00:00,  3.46it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TRAIN MODEL\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    train_results = train(model, train_loader, loss_weights = loss_weights)\n",
    "    val_results = eval(model, val_loader, loss_weights = loss_weights)\n",
    "    \n",
    "    train_mask_losses.append(train_results['loss_mask'])\n",
    "    train_phase_losses.append(train_results['loss_phase'])\n",
    "    train_cod_losses.append(train_results['loss_cod'])\n",
    "    train_cps_losses.append(train_results['loss_cps'])\n",
    "    train_all_losses.append(train_results['loss_total'])\n",
    "\n",
    "    train_mask_acc.append(train_results['acc_mask'])\n",
    "    train_phase_acc.append(train_results['acc_phase'])\n",
    "    train_cod_r2.append(train_results['r2_cod'])\n",
    "    train_cps_r2.append(train_results['r2_cps'])\n",
    "\n",
    "    val_mask_losses.append(val_results['loss_mask'])\n",
    "    val_phase_losses.append(val_results['loss_phase'])\n",
    "    val_cod_losses.append(val_results['loss_cod'])\n",
    "    val_cps_losses.append(val_results['loss_cps'])\n",
    "    val_all_losses.append(val_results['loss_total'])\n",
    "\n",
    "    val_mask_acc.append(val_results['acc_mask'])\n",
    "    val_phase_acc.append(val_results['acc_phase'])\n",
    "    val_cod_r2.append(val_results['r2_cod'])\n",
    "    val_cps_r2.append(val_results['r2_cps'])\n",
    "\n",
    "    print(f\"Epoch: {e} | Train Loss: {train_results['loss_total']:.4f} | Val Loss: {val_results['loss_total']:.4f} | Val Mask: {val_results['acc_mask']:.4f} | Val Phase: {val_results['acc_phase']:.4f} | Val Cod: {val_results['r2_cod']:.4f} | Val Cps: {val_results['r2_cps']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9b63f-6056-44f4-9be1-1aad91b96477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT LOSS\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(train_mask_losses, label='Train Loss')\n",
    "plt.plot(val_mask_losses, label='Val Loss')\n",
    "plt.title('Cloud Mask Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(train_phase_losses, label='Train Loss')\n",
    "plt.plot(val_phase_losses, label='Val Loss')\n",
    "plt.title('Cloud Phase Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(train_cod_losses, label='Train Loss')\n",
    "plt.plot(val_cod_losses, label='Val Loss')\n",
    "plt.title('Cod Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(train_cps_losses, label='Train Loss')\n",
    "plt.plot(val_cps_losses, label='Val Loss')\n",
    "plt.title('Cps Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(train_all_losses, label='Train Loss')\n",
    "plt.plot(val_all_losses, label='Val Loss')\n",
    "plt.title('All Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"./graphs/MultiTaskV3.1_loss.png\")  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e46110-6b35-4d63-89cc-5717ef84e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT ACCURACY\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_mask_acc, label='Train Acc')\n",
    "plt.plot(val_mask_acc, label='Val Acc')\n",
    "plt.title('Cloud Mask Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_phase_acc, label='Train Acc')\n",
    "plt.plot(val_phase_acc, label='Val Acc')\n",
    "plt.title('Cloud Phase Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_cod_r2, label='Train R2')\n",
    "plt.plot(val_cod_r2, label='Val R2')\n",
    "plt.title('Cod R2')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_cps_r2, label='Train R2')\n",
    "plt.plot(val_cps_r2, label='Val R2')\n",
    "plt.title('Cps R2')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"./graphs/MultiTaskV3.1_acc.png\")  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01ec0a-c88e-4822-b121-21e305efa5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION — CLOUD MASK\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        cloud_mask_target, _, _, _ = unpack_labels(labels)\n",
    "        cloud_mask_pred, _, _, _ = model(images)\n",
    "\n",
    "        probs = torch.sigmoid(cloud_mask_pred)\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(cloud_mask_target.cpu().numpy())\n",
    "\n",
    "# Flatten predictions and labels\n",
    "all_preds = np.concatenate([p.flatten() for p in all_preds])\n",
    "all_labels = np.concatenate([l.flatten() for l in all_labels])\n",
    "\n",
    "# Classification report and IoU\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    labels=[0, 1],\n",
    "    digits=3,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "f1_scores = np.array([report[str(i)]['f1-score'] for i in range(2)])\n",
    "supports = np.array([report[str(i)]['support'] for i in range(2)])\n",
    "iou = f1_scores / (2 - f1_scores)\n",
    "\n",
    "\n",
    "# Output\n",
    "print(\"CLOUD MASK REPORT:\\n\", classification_report(all_labels, all_preds, labels=[0, 1], digits=3))\n",
    "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(all_labels, all_preds, labels=[0, 1]))\n",
    "print(\"\\nIOU:\", iou)\n",
    "print(\"Unweighted IoU:\", np.mean(iou))\n",
    "print(\"Weighted IoU:\", np.average(iou, weights=supports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a14322-3e69-4a28-b34c-21ebb0372a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION — CLOUD PHASE\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        _, cloud_phase_target, _, _ = unpack_labels(labels)\n",
    "        _, cloud_phase_pred, _, _ = model(images)\n",
    "        preds = torch.argmax(cloud_phase_pred, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(cloud_phase_target.cpu().numpy())\n",
    "        \n",
    "# Flatten predictions and labels\n",
    "all_preds = np.concatenate([p.flatten() for p in all_preds])\n",
    "all_labels = np.concatenate([l.flatten() for l in all_labels])\n",
    "\n",
    "# Classification report and IoU\n",
    "report = classification_report(all_labels, all_preds, digits=3, output_dict=True)\n",
    "f1_scores = np.array([report[str(i)]['f1-score'] for i in range(num_classes)])\n",
    "supports = np.array([report[str(i)]['support'] for i in range(num_classes)])\n",
    "iou = f1_scores / (2 - f1_scores)\n",
    "\n",
    "# Output\n",
    "print(\"CLOUD PHASE REPORT:\\n\", classification_report(all_labels, all_preds, digits=3))\n",
    "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(all_labels, all_preds))\n",
    "print(\"\\nIOU:\", iou)\n",
    "print(\"Unweighted IoU:\", np.mean(iou))\n",
    "print(\"Weighted IoU:\", np.average(iou, weights=supports))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdea56d-5534-4e12-9666-1becdfa2ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION — CLOUD OPTICAL DISTANCE\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        _, _, cod_target, _ = unpack_labels(labels)\n",
    "        _, _, cod_pred, _ = model(images)\n",
    "        \n",
    "        all_preds.append(cod_pred.cpu().numpy().reshape(-1))\n",
    "        all_labels.append(cod_target.cpu().numpy().reshape(-1))\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"r2:\", r2_score(all_labels, all_preds))\n",
    "print(\"MSE:\", mean_squared_error(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a4a78-6c7a-410c-b734-b58984a85f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION — CLOUD PARTICLE SIZE\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        _, _, _, cps_target = unpack_labels(labels)\n",
    "        _, _, _, cps_pred = model(images)\n",
    "        \n",
    "        all_preds.append(cps_pred.cpu().numpy().reshape(-1))\n",
    "        all_labels.append(cps_target.cpu().numpy().reshape(-1))\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"r2:\", r2_score(all_labels, all_preds))\n",
    "print(\"MSE:\", mean_squared_error(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845b251-8b71-407b-9e94-f52549f51b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_image = images[0].cpu().numpy()\n",
    "# my_image = np.transpose(my_image, (1, 2, 0))\n",
    "# phase_pred = preds[0].cpu().numpy()\n",
    "# phase_target = cloud_phase_target[0].cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# from netCDF4 import Dataset\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# with Dataset('./maps/image1.nc', 'w', format='NETCDF4') as ds:\n",
    "#     ds.createDimension('x', my_image.shape[0])\n",
    "#     ds.createDimension('y', my_image.shape[1])\n",
    "#     ds.createDimension('band', my_image.shape[2])\n",
    "\n",
    "#     var = ds.createVariable('radiance', 'f4', ('x', 'y', 'band'))\n",
    "#     var[:] = my_image\n",
    "\n",
    "#     var.units = 'unknown'  # optional metadata\n",
    "\n",
    "# with Dataset('./maps/image2.nc', 'w', format='NETCDF4') as ds:\n",
    "#     ds.createDimension('x', phase_pred.shape[0])\n",
    "#     ds.createDimension('y', phase_pred.shape[1])\n",
    "\n",
    "#     var = ds.createVariable('prediction', 'f4', ('x', 'y'))\n",
    "#     var[:] = phase_pred\n",
    "\n",
    "#     var.units = 'unknown'  # optional metadata\n",
    "\n",
    "\n",
    "# with Dataset('./maps/image3.nc', 'w', format='NETCDF4') as ds:\n",
    "#     ds.createDimension('x', phase_target.shape[0])\n",
    "#     ds.createDimension('y', phase_target.shape[1])\n",
    "\n",
    "#     var = ds.createVariable('target', 'f4', ('x', 'y'))\n",
    "#     var[:] = phase_target\n",
    "\n",
    "#     var.units = 'unknown'  # optional metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc2641-a17b-41c8-863e-65a6ffe6660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_mask_losses, label='Train Loss')\n",
    "plt.plot(val_mask_losses, label='Val Loss')\n",
    "plt.title('Cloud Mask Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, max(max(train_mask_losses), max(val_mask_losses)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b82439-b5b6-44b4-b814-e539811c5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to CSV (create if doesn't exist, no header if appending)\n",
    "csv_path = \"./Version_results.csv\"\n",
    "try:\n",
    "    existing = pd.read_csv(csv_path, index_col=0)\n",
    "    existing.loc[\"V3.1 Cloud mask\"] = val_mask_losses\n",
    "    existing.to_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2103e0-432f-46b1-9d6d-fbfdfcaaafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./HPT_results.csv\"\n",
    "try:\n",
    "    existing = pd.read_csv(csv_path, index_col=0)\n",
    "    existing.loc[\"Weights = (1, 1, 1, 1\"] = val_mask_losses\n",
    "    existing.to_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bf55a-5927-47ce-99fe-a94179b9d967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
